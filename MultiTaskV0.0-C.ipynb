{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent Learning of Semantic Relations\n",
    "## Authors : hidden\n",
    "\n",
    "Notebook to reproduce the results of the paper with respect to identifying the semantic relations between words.\n",
    "\n",
    "Note: the results may not be identical to those put to the paper due to randomization; the differences in any case should be very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "import numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# np.random.seed(44)\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read the data and perform the lexical split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_data='Rumen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Function loads the dataset and returns 3 dataframes df1 for pairs of task1,df2 for task2 and df3 for random pairs\n",
    "def three_data(name_data):\n",
    "    #Perpare_data for BLess or Root9\n",
    "    Rumen,Root9,Bless,Cogalex,Weeds=False,False,False,False,False\n",
    "    if name_data=='Rumen':\n",
    "        Rumen=True\n",
    "    if name_data=='Root9':\n",
    "        Root9=True\n",
    "    if name_data=='Bless':\n",
    "        Bless=True\n",
    "    if name_data =='Cogalex':\n",
    "        Cogalex=True\n",
    "    if name_data =='Weeds':\n",
    "        Weeds =True\n",
    "        \n",
    "    def prep_df(df):\n",
    "        cols =df.columns\n",
    "        for i in range(len(cols)) :\n",
    "            for j in range(len(df)):\n",
    "                df[df.columns[i]].values[j]=df[df.columns[i]].values[j][0:-2]\n",
    "        return df\n",
    "    if Weeds:\n",
    "        Rumen=True\n",
    "        task1=\"HYPER\"\n",
    "        task2=\"Coord\"\n",
    "        link2=\"data/coordpairs2_wiki100.json\"\n",
    "        link1=\"data/entpairs2_wiki100.json\"\n",
    "    if Cogalex :\n",
    "        task1=\"HYPER\"\n",
    "        task2=\"SYN\"\n",
    "        link1=\"CogALexV_train_v1/gold_task2.txt\"\n",
    "        link2=\"CogALexV_test_v1/gold_task2.txt\"\n",
    "    if Rumen :\n",
    "        task1=\"HYPER\"\n",
    "        task2=\"SYN\"\n",
    "        link=\"RumenPairs.txt\"\n",
    "    if Root9 :\n",
    "        link_hyper=\"MULTITASK-LEARNING/ROOT9/ROOT9_hyper.txt\"\n",
    "        link_coord=\"MULTITASK-LEARNING/ROOT9/ROOT9_coord.txt\"\n",
    "        link_random=\"MULTITASK-LEARNING/ROOT9/ROOT9_random.txt\"\n",
    "        task1= \"HYPER\"\n",
    "        task2= \"COORD\"\n",
    "    if Bless :\n",
    "        task1= \"HYPER\"\n",
    "        task2= \"MERO\"\n",
    "        link_coord=\"MULTITASK-LEARNING/BLESS/BLESS_mero.txt\"\n",
    "        link_hyper=\"MULTITASK-LEARNING/BLESS/BLESS_hyper.txt\"\n",
    "        link_random=\"MULTITASK-LEARNING/BLESS/BLESS_random.txt\"\n",
    "\n",
    "\n",
    "    def get_names(cat):\n",
    "        if cat == 0 : return \"RANDOM\"\n",
    "        if cat == 1: return task1\n",
    "        if cat == 2: return task2\n",
    "    def get_names_Weeds1(cat):\n",
    "        if cat == 0 : return \"RANDOM\"\n",
    "        if cat == 1: return task1\n",
    "    def get_names_Weeds2(cat):\n",
    "        if cat == 0 : return \"RANDOM\"\n",
    "        if cat == 1: return task2\n",
    "\n",
    "\n",
    "    if Rumen :\n",
    "        dff = pd.read_csv(link)\n",
    "        dff.rename(columns={\"W1\":\"w1\", \"W2\":\"w2\",\"rel\":\"Category\"}, inplace=True)\n",
    "        dff[\"Category\"] = dff[\"Category\"].apply(get_names)\n",
    "        df = dff.loc[dff.Category == task2]\n",
    "        df2 = dff.loc[dff.Category == task1]\n",
    "        df3 = dff.loc[dff.Category == \"RANDOM\"]\n",
    "        #print(len(df),len(df2),len(df3))\n",
    "    if Root9 or Bless:\n",
    "\n",
    "        df = pd.read_csv(link_coord,header=None,sep = '\\t')\n",
    "        df.rename(index=str,columns={0:\"w1\", 2:\"w2\",1:\"Category\"},inplace=True)\n",
    "        df=prep_df(df)\n",
    "        df2 = pd.read_csv(link_hyper,header=None,sep = '\\t')\n",
    "        df2.rename(index=str,columns={0:\"w1\", 2:\"w2\",1:\"Category\"},inplace=True)\n",
    "        df2=prep_df(df2)\n",
    "        df3 = pd.read_csv(link_random,header=None,sep = '\\t')\n",
    "        df3.rename(index=str,columns={0:\"w1\", 2:\"w2\",1:\"Category\"},inplace=True)\n",
    "        df3=prep_df(df3)\n",
    "    if Cogalex:\n",
    "\n",
    "        dff1 = pd.read_csv(link1,header=None,sep = '\\t')\n",
    "        dff2 = pd.read_csv(link2,header=None,sep = '\\t')\n",
    "        dff1.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff2.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff3=pd.concat([dff1,dff2])\n",
    "        #print(list(set(dff3.Category.values.tolist())))\n",
    "        df = dff3.loc[dff3.Category == task2]\n",
    "        df2 = dff3.loc[dff3.Category == task1]\n",
    "        df3 = dff3.loc[dff3.Category == \"RANDOM\"]\n",
    "    if Weeds :\n",
    "        json_data=open(link1).read()\n",
    "        data = json.loads(json_data)\n",
    "        dff=pd.DataFrame(data)\n",
    "        dff.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff[\"Category\"] = dff[\"Category\"].apply(get_names_Weeds1)\n",
    "        df2 = dff.loc[dff.Category == task1]\n",
    "        #df3 = dff.loc[dff.Category == \"RANDOM\"]\n",
    "        #df3=df3[0:len(df2)]\n",
    "        #print(\"taille 0,1 pour entpairs\",len(df2),len(df3))\n",
    "        \n",
    "        json_data2=open(link2).read()\n",
    "        data2 = json.loads(json_data2)\n",
    "        dff2=pd.DataFrame(data2)\n",
    "        dff2.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff2[\"Category\"] = dff2[\"Category\"].apply(get_names_Weeds2)\n",
    "        df = dff2.loc[dff2.Category == task2]\n",
    "        #df=df[0:len(df2)]\n",
    "        #print(\"taille 0,1 pour coord\",len(df))\n",
    "        \n",
    "        \n",
    "    return df,df2,df3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df2,df3=three_data(name_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_coord = list(set(df.w1.values.tolist() + df.w2.values.tolist()))\n",
    "words_hyper = list(set(df2.w1.values.tolist() + df2.w2.values.tolist()))\n",
    "words_rando = list(set(df3.w1.values.tolist() + df3.w2.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and mark the words that are in the embeddings dictionary\n",
    "\n",
    "def get_vector_representation_of_word_pairs(dataframe, embeddings_voca):\n",
    "    x1 = [embeddings_voca[word] for word in dataframe.w1.values]\n",
    "    x2 =[embeddings_voca[word] for word in dataframe.w2.values]\n",
    "    y = dataframe.Category.values\n",
    "    #Concatenation\n",
    "    x = np.hstack((x1, x2))\n",
    "    return x, y\n",
    "\n",
    "def load_embeddings(path, dimension):\n",
    "    f = open(path, encoding=\"utf8\").read().splitlines()\n",
    "    vectors = {}\n",
    "    for i in f:\n",
    "        elems = i.split()\n",
    "        vectors[\" \".join(elems[:-dimension])] =  np.array(elems[-dimension:]).astype(float)\n",
    "    return vectors\n",
    "\n",
    "embeddings = load_embeddings(\"glove.6B.300d.txt\", 300)\n",
    "df[\"known_words\"] = df.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )\n",
    "df2[\"known_words\"] = df2.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )\n",
    "df3[\"known_words\"] = df3.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6326, 957, 2256, 3213)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the lexical split using the vocabulary of the corpus\n",
    "\n",
    "words_ = sorted(list(set(words_coord+words_hyper+words_rando)))\n",
    "words_train, words_test =train_test_split(words_, test_size=0.4, random_state=1344)\n",
    "\n",
    "# Given the words in the train and test parts, mark the pairs as training or testing, when both words of aa pair belong to the train or test vocabulary.\n",
    "df[\"is_train\"] = df.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "df[\"is_test\"] = df.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    "\n",
    "df2[\"is_train\"] = df2.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "df2[\"is_test\"] = df2.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    "\n",
    "df3[\"is_train\"] = df3.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "df3[\"is_test\"] = df3.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    "\n",
    "\n",
    "df.shape[0], df.is_test.astype(int).sum(), df.is_train.astype(int).sum(), df.is_test.astype(int).sum() + df.is_train.astype(int).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the inputs of the learning systems (concatenation of GloVe embeddings)\n",
    "xtrainCoord, ytrainCoord = get_vector_representation_of_word_pairs(df.loc[df.is_train==True], embeddings)\n",
    "xtestCoord, ytestCoord   = get_vector_representation_of_word_pairs(df.loc[df.is_test==True], embeddings)\n",
    "\n",
    "xtrainHyper, ytrainHyper = get_vector_representation_of_word_pairs(df2.loc[df2.is_train==True], embeddings)\n",
    "xtestHyper, ytestHyper   = get_vector_representation_of_word_pairs(df2.loc[df2.is_test==True], embeddings)\n",
    "\n",
    "xtrainRando, ytrainRando = get_vector_representation_of_word_pairs(df3.loc[df3.is_train==True], embeddings) \n",
    "xtestRando, ytestRando   = get_vector_representation_of_word_pairs(df3.loc[df3.is_test==True], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4483 4865\n"
     ]
    }
   ],
   "source": [
    "x_train_1, x_train_2 = np.vstack((xtrainCoord, xtrainRando)), np.vstack((xtrainHyper, xtrainRando))\n",
    "y_train_1, y_train_2 = [1]*len(xtrainCoord) + [0]*len(xtrainRando), [1]*len(xtrainHyper) + [0]*len(xtrainRando)\n",
    "\n",
    "\n",
    "x_test_1, x_test_2 = np.vstack((xtestCoord, xtestRando)), np.vstack((xtestHyper, xtestRando))\n",
    "y_test_1, y_test_2 = [1]*len(xtestCoord) + [0]*len(xtestRando), [1]*len(xtestHyper) + [0]*len(xtestRando)\n",
    "\n",
    "x_train_1, y_train_1 = shuffle(x_train_1, y_train_1, random_state=1234)\n",
    "x_train_2, y_train_2 = shuffle(x_train_2, y_train_2, random_state=1234)\n",
    "x_test_1, y_test_1 = shuffle(x_test_1, y_test_1, random_state=1234)\n",
    "x_test_2, y_test_2 = shuffle(x_test_2, y_test_2, random_state=1234)\n",
    "assert len(x_train_1) == len(y_train_1)\n",
    "assert len(x_train_2) == len(y_train_2)\n",
    "assert len(x_test_1) == len(y_test_1)\n",
    "assert len(x_test_2) == len(y_test_2)\n",
    "print(len(x_train_1), len(x_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_my_nn_model():\n",
    "    \"\"\"Defines the NN baseline.\n",
    "    Two hidden layers, followed by the output layer. \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, activation='sigmoid', input_dim=600))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def get_n_most_confident_predictions(my_preds, n, y_train_data, x_unlabeled):\n",
    "    \"\"\"\n",
    "    defines the function used for self-learning\n",
    "    It selects 10 \n",
    "    \"\"\"\n",
    "    y_train_data = np.array(y_train_data)\n",
    "    probabilities = np.array([(y_train_data==0).sum()/y_train_data.shape[0], (y_train_data==1).sum()/y_train_data.shape[0]])\n",
    "    probabilities = (probabilities * n).astype(int) \n",
    "    new_x, new_y, exclude_indices = [], [], []\n",
    "    my_preds  = my_preds.reshape(1,-1)[0]\n",
    "    negative_to_positive = np.argsort(my_preds)\n",
    "    new_x.extend(x_unlabeled[negative_to_positive][:probabilities[0]])\n",
    "    new_x.extend(x_unlabeled[negative_to_positive][-probabilities[1]:])\n",
    "    new_y.extend([0]*probabilities[0])\n",
    "    new_y.extend([1]*probabilities[1])\n",
    "    exclude_indices.extend(negative_to_positive[:probabilities[0]])\n",
    "    exclude_indices.extend(negative_to_positive[-probabilities[1]:])\n",
    "    updated_unlabel_indices = np.setdiff1d(np.arange(len(x_unlabeled)), exclude_indices)  \n",
    "#     print(\"Returning for self-training:\", probabilities.sum(), \"Unlabeled. Had: %d, now have: %d\"%(len(x_unlabeled), len(updated_unlabel_indiced)))\n",
    "    return np.array(new_x), np.array(new_y), x_unlabeled[updated_unlabel_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_my_multitask_nn_models():\n",
    "    inputs = Input(shape=(600,))\n",
    "    x = Dense(50, activation='sigmoid')(inputs)\n",
    "\n",
    "    coord = Dense(1, activation='sigmoid', name='coord_output')(x)\n",
    "    hyper = Dense(1, activation='sigmoid', name='hyper_output')(x)\n",
    "\n",
    "    model_coord = Model(inputs=[inputs], outputs=[coord])\n",
    "    model_hyper = Model(inputs=[inputs], outputs=[hyper])\n",
    "\n",
    "    model_coord.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    model_hyper.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    return model_coord, model_hyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the results may not be identical to those put to the paper due to randomization; the differences in any case should be very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Coord-Random \n",
      "Basic Stats: 1255 538 2690 1926 \n",
      "Majority classifier: 0.4968847352024922 0.33194588969823097 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.6282450674974039 0.6282434640082384 \n",
      "NN baseline: 0.6874350986500519 0.6873376623376624 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN baseline + self-learning 0.6910695742471443 0.6910554990460482\n",
      "Problem Hyper-Random \n",
      "Basic Stats: 1362 584 2919 1706 \n",
      "Majority classifier: 0.4320046893317702 0.30167826442898077 \n",
      "Logistic Regression: 0.7110199296600235 0.709032348459033 \n",
      "NN baseline: 0.7508792497069168 0.7474428171594676 \n",
      "NN baseline + self-learning 0.7549824150058617 0.7520348837209303\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for name, x_train, y_train, x_test, y_test in zip([\"Coord-Random\", \"Hyper-Random\"], [x_train_1, x_train_2], [y_train_1, y_train_2], [x_test_1, x_test_2], [y_test_1, y_test_2]):   \n",
    "    # Perform the splits in train, validation, unlabeled\n",
    "    x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train, stratify=y_train, test_size=0.6, random_state=1234,)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, stratify=y_train,  test_size=0.30, random_state=1234,)\n",
    "    print(\"Problem\", name, \"\\nBasic Stats:\", len(x_train), len(x_valid), len(x_unlabeled), len(x_test), end=\" \\n\")\n",
    "\n",
    "    # Start with the accuracy and MaF1 scores of a Dummy Classifier\n",
    "    clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_test)\n",
    "    print(\"Majority classifier:\", accuracy_score(y_test, preds), f1_score(y_test, preds, average=\"macro\"), end=\" \\n\")\n",
    "\n",
    "    \n",
    "    # Logistic Regression with default params\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_test)\n",
    "    print(\"Logistic Regression:\", accuracy_score(y_test, preds), f1_score(y_test, preds, average=\"macro\"), end=\" \\n\")\n",
    "    \n",
    "    # NN baseline\n",
    "    model = get_my_nn_model()\n",
    "    model.fit(x_train, y_train, epochs=500, validation_data=(x_valid, y_valid), verbose=False, callbacks=[EarlyStopping(patience=5)])\n",
    "    preds = model.predict_classes(x_test, verbose=False)\n",
    "    print(\"NN baseline:\", accuracy_score(y_test, preds), f1_score(y_test, preds, average=\"macro\"), end=\" \\n\")\n",
    "    \n",
    "    # keep the train/validation/test splits so that hey can be used with multitask learning and the results are comparable between them\n",
    "    data[name]={\"x_train\": x_train, \"y_train\":y_train, \"x_unlabeled\":x_unlabeled, \"y_unlabeled\":y_unlabeled,  \"x_valid\":x_unlabeled, \"y_valid\":y_unlabeled, \"x_test\":x_test,  \"y_test\":y_test } \n",
    "    \n",
    "    # NN baseline + self learning \n",
    "    accuracy_scores, mif_scores = [], []\n",
    "    for i in range(20):\n",
    "        tmpx, tmpy, x_unlabeled = get_n_most_confident_predictions(model.predict(x_unlabeled), 10, y_train, x_unlabeled) # use 10 labels each time\n",
    "        x_train = np.vstack((x_train, tmpx))\n",
    "        y_train = np.concatenate((y_train, tmpy))\n",
    "        model = get_my_nn_model()\n",
    "        model.fit(x_train, y_train, nb_epoch=500, validation_data=(x_valid, y_valid), verbose=False, \\\n",
    "                  callbacks=[EarlyStopping(patience=5)],)\n",
    "        preds = model.predict_classes(x_test, verbose=0)\n",
    "        accuracy_scores.append(accuracy_score(y_test, preds))# , f1_score(y_test, preds, average=\"macro\"))        \n",
    "        mif_scores.append( f1_score(y_test, preds, average=\"macro\") )\n",
    "    \n",
    "    print(\"NN baseline + self-learning\", max(accuracy_scores), max(mif_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multitask (Coord-Random): 0.6998961578400831 0.699849551079835 \n",
      "Multitask (Hyper-Random): 0.757327080890973 0.7542139191222631\n"
     ]
    }
   ],
   "source": [
    "# Repeat the process with multi-task learning\n",
    "model_coord, model_hyper = get_my_multitask_nn_models()\n",
    "\n",
    "acc_scores_coord,mif_scores_coord, acc_scores_hyper,  mif_scores_hyper= [], [], [], []\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    model_coord.fit(data[\"Coord-Random\"][\"x_train\"], data[\"Coord-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "    model_hyper.fit(data[\"Hyper-Random\"][\"x_train\"], data[\"Hyper-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "    preds_coord = (model_coord.predict(data[\"Coord-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "    preds_hyper = (model_hyper.predict(data[\"Hyper-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "    preds_coord_valid = (model_coord.predict(data[\"Coord-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "    preds_hyper_valid = (model_hyper.predict(data[\"Hyper-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "    acc_scores_coord.append([accuracy_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid), accuracy_score(data[\"Coord-Random\"][\"y_test\"], preds_coord)])\n",
    "    acc_scores_hyper.append([accuracy_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid), accuracy_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper)])\n",
    "    mif_scores_coord.append([f1_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid, average=\"macro\") , f1_score(data[\"Coord-Random\"][\"y_test\"], preds_coord, average=\"macro\")])\n",
    "    mif_scores_hyper.append([f1_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid, average=\"macro\") , f1_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper, average=\"macro\")])\n",
    "\n",
    "# Monitor validation score in the score tuple and print accordingly\n",
    "print(\"Multitask (Coord-Random):\", max(acc_scores_coord)[1], max(mif_scores_coord)[1], \"\\nMultitask (Hyper-Random):\",  max(acc_scores_hyper)[1], max(mif_scores_hyper)[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multitask + Self-learning(Coord-Random): 0.705607476635514 0.7055978734847432 \n",
      "Multitask + Self-learning(Hyper-Random): 0.7608440797186401 0.7573054393305438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_scores_coord,mif_scores_coord, acc_scores_hyper,  mif_scores_hyper= [], [], [], []\n",
    "for i in range(20):\n",
    "    tmpx, tmpy, data[\"Coord-Random\"][\"x_unlabeled\"] = get_n_most_confident_predictions(model_coord.predict(data[\"Coord-Random\"][\"x_unlabeled\"]), 10, data[\"Coord-Random\"][\"y_train\"], data[\"Coord-Random\"][\"x_unlabeled\"])\n",
    "    data[\"Coord-Random\"][\"x_train\"] = np.vstack((data[\"Coord-Random\"][\"x_train\"], tmpx))\n",
    "    data[\"Coord-Random\"][\"y_train\"] = np.concatenate((data[\"Coord-Random\"][\"y_train\"], tmpy))\n",
    "\n",
    "    tmpx, tmpy, data[\"Hyper-Random\"][\"x_unlabeled\"] = get_n_most_confident_predictions(model_hyper.predict(data[\"Hyper-Random\"][\"x_unlabeled\"]), 10, data[\"Hyper-Random\"][\"y_train\"], data[\"Hyper-Random\"][\"x_unlabeled\"])\n",
    "    data[\"Hyper-Random\"][\"x_train\"] = np.vstack((data[\"Hyper-Random\"][\"x_train\"], tmpx))\n",
    "    data[\"Hyper-Random\"][\"y_train\"] = np.concatenate((data[\"Hyper-Random\"][\"y_train\"], tmpy))\n",
    "    acc_scores_coord_, mif_scores_coord_, acc_scores_hyper_,  mif_scores_hyper_ = [], [], [], []\n",
    "\n",
    "    model_coord, model_hyper = get_my_multitask_nn_models()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model_coord.fit(data[\"Coord-Random\"][\"x_train\"], data[\"Coord-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "        model_hyper.fit(data[\"Hyper-Random\"][\"x_train\"], data[\"Hyper-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "        preds_coord = (model_coord.predict(data[\"Coord-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "        preds_hyper = (model_hyper.predict(data[\"Hyper-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "        preds_coord_valid = (model_coord.predict(data[\"Coord-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "        preds_hyper_valid = (model_hyper.predict(data[\"Hyper-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "        acc_scores_coord_.append([accuracy_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid), accuracy_score(data[\"Coord-Random\"][\"y_test\"], preds_coord)])\n",
    "        acc_scores_hyper_.append([accuracy_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid), accuracy_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper)])\n",
    "        mif_scores_coord_.append([f1_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid, average=\"macro\") , f1_score(data[\"Coord-Random\"][\"y_test\"], preds_coord, average=\"macro\")])\n",
    "        mif_scores_hyper_.append([f1_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid, average=\"macro\") , f1_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper, average=\"macro\")])\n",
    "\n",
    "\n",
    "    acc_scores_coord.append(max(acc_scores_coord_))\n",
    "    mif_scores_coord.append(max(mif_scores_coord_))\n",
    "    acc_scores_hyper.append(max(acc_scores_hyper_))\n",
    "    mif_scores_hyper.append(max(mif_scores_hyper_))\n",
    "print(\"\\nMultitask + Self-learning(Coord-Random):\", max(acc_scores_coord)[1], max(mif_scores_coord)[1], \"\\nMultitask + Self-learning(Hyper-Random):\",  max(acc_scores_hyper)[1],  max(mif_scores_hyper)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3.6",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
