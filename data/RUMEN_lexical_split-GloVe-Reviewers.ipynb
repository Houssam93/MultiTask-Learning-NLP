{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent Learning of Semantic Relations\n",
    "## Authors : hidden\n",
    "\n",
    "Notebook to reproduce the results of the paper with respect to identifying the semantic relations between words.\n",
    "\n",
    "Note: the results may not be identical to those put to the paper due to randomization; the differences in any case should be very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "import numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# np.random.seed(44)\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read the data and perform the lexical split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dff = pd.read_csv(\"./RUMEN/RumenPairs.txt\")\n",
    "dff.rename(columns={\"W1\":\"w1\", \"W2\":\"w2\",\"rel\":\"Category\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_names(cat):\n",
    "    if cat == 0 : return \"RANDOM\"\n",
    "    if cat == 1: return \"HYPER\"\n",
    "    if cat == 2: return \"SYN\"\n",
    "dff[\"Category\"] = dff[\"Category\"].apply(get_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dff.loc[dff.Category == \"SYN\"]\n",
    "df2 = dff.loc[dff.Category == \"HYPER\"]\n",
    "df3 = dff.loc[dff.Category == \"RANDOM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_coord = list(set(df.w1.values.tolist() + df.w2.values.tolist()))\n",
    "words_hyper = list(set(df2.w1.values.tolist() + df2.w2.values.tolist()))\n",
    "words_rando = list(set(df3.w1.values.tolist() + df3.w2.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load embeddings and mark the words that are in the embeddings dictionary\n",
    "\n",
    "def get_vector_representation_of_word_pairs(dataframe, embeddings_voca):\n",
    "    x1 = [embeddings_voca[word] for word in dataframe.w1.values]\n",
    "    x2 =[embeddings_voca[word] for word in dataframe.w2.values]\n",
    "    y = dataframe.Category.values\n",
    "    x = np.hstack((x1, x2))\n",
    "    return x, y\n",
    "\n",
    "def load_embeddings(path, dimension):\n",
    "    f = open(path, encoding=\"utf8\").read().splitlines()\n",
    "    vectors = {}\n",
    "    for i in f:\n",
    "        elems = i.split()\n",
    "        vectors[\" \".join(elems[:-dimension])] =  np.array(elems[-dimension:]).astype(float)\n",
    "    return vectors\n",
    "\n",
    "embeddings = load_embeddings(\"./glove.6B.300d.txt\", 300)\n",
    "df[\"known_words\"] = df.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )\n",
    "df2[\"known_words\"] = df2.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )\n",
    "df3[\"known_words\"] = df3.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform the lexical split using the vocabulary of the corpus\n",
    "\n",
    "words_ = sorted(list(set(words_coord+words_hyper+words_rando)))\n",
    "words_train, words_test =train_test_split(words_, test_size=0.4, random_state=1344)\n",
    "\n",
    "# Given the words in the train and test parts, mark the pairs as training or testing, when both words of aa pair belong to the train or test vocabulary.\n",
    "df[\"is_train\"] = df.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "df[\"is_test\"] = df.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    "\n",
    "df2[\"is_train\"] = df2.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "df2[\"is_test\"] = df2.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    "\n",
    "df3[\"is_train\"] = df3.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "df3[\"is_test\"] = df3.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    "\n",
    "\n",
    "df.shape[0], df.is_test.astype(int).sum(), df.is_train.astype(int).sum(), df.is_test.astype(int).sum() + df.is_train.astype(int).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the inputs of the learning systems (concatenation of GloVe embeddings)\n",
    "xtrainCoord, ytrainCoord = get_vector_representation_of_word_pairs(df.loc[df.is_train==True], embeddings)\n",
    "xtestCoord, ytestCoord   = get_vector_representation_of_word_pairs(df.loc[df.is_test==True], embeddings)\n",
    "\n",
    "xtrainHyper, ytrainHyper = get_vector_representation_of_word_pairs(df2.loc[df2.is_train==True], embeddings)\n",
    "xtestHyper, ytestHyper   = get_vector_representation_of_word_pairs(df2.loc[df2.is_test==True], embeddings)\n",
    "\n",
    "xtrainRando, ytrainRando = get_vector_representation_of_word_pairs(df3.loc[df3.is_train==True], embeddings) \n",
    "xtestRando, ytestRando   = get_vector_representation_of_word_pairs(df3.loc[df3.is_test==True], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_1, x_train_2 = np.vstack((xtrainCoord, xtrainRando)), np.vstack((xtrainHyper, xtrainRando))\n",
    "y_train_1, y_train_2 = [1]*len(xtrainCoord) + [0]*len(xtrainRando), [1]*len(xtrainHyper) + [0]*len(xtrainRando)\n",
    "\n",
    "\n",
    "x_test_1, x_test_2 = np.vstack((xtestCoord, xtestRando)), np.vstack((xtestHyper, xtestRando))\n",
    "y_test_1, y_test_2 = [1]*len(xtestCoord) + [0]*len(xtestRando), [1]*len(xtestHyper) + [0]*len(xtestRando)\n",
    "\n",
    "x_train_1, y_train_1 = shuffle(x_train_1, y_train_1, random_state=1234)\n",
    "x_train_2, y_train_2 = shuffle(x_train_2, y_train_2, random_state=1234)\n",
    "x_test_1, y_test_1 = shuffle(x_test_1, y_test_1, random_state=1234)\n",
    "x_test_2, y_test_2 = shuffle(x_test_2, y_test_2, random_state=1234)\n",
    "assert len(x_train_1) == len(y_train_1)\n",
    "assert len(x_train_2) == len(y_train_2)\n",
    "assert len(x_test_1) == len(y_test_1)\n",
    "assert len(x_test_2) == len(y_test_2)\n",
    "print(len(x_train_1), len(x_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_my_nn_model():\n",
    "    \"\"\"Defines the NN baseline.\n",
    "    Two hidden layers, followed by the output layer. \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, activation='sigmoid', input_dim=600))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def get_n_most_confident_predictions(my_preds, n, y_train_data, x_unlabeled):\n",
    "    \"\"\"\n",
    "    defines the function used for self-learning\n",
    "    It selects 10 \n",
    "    \"\"\"\n",
    "    y_train_data = np.array(y_train_data)\n",
    "    probabilities = np.array([(y_train_data==0).sum()/y_train_data.shape[0], (y_train_data==1).sum()/y_train_data.shape[0]])\n",
    "    probabilities = (probabilities * n).astype(int) \n",
    "    new_x, new_y, exclude_indices = [], [], []\n",
    "    my_preds  = my_preds.reshape(1,-1)[0]\n",
    "    negative_to_positive = np.argsort(my_preds)\n",
    "    new_x.extend(x_unlabeled[negative_to_positive][:probabilities[0]])\n",
    "    new_x.extend(x_unlabeled[negative_to_positive][-probabilities[1]:])\n",
    "    new_y.extend([0]*probabilities[0])\n",
    "    new_y.extend([1]*probabilities[1])\n",
    "    exclude_indices.extend(negative_to_positive[:probabilities[0]])\n",
    "    exclude_indices.extend(negative_to_positive[-probabilities[1]:])\n",
    "    updated_unlabel_indices = np.setdiff1d(np.arange(len(x_unlabeled)), exclude_indices)  \n",
    "#     print(\"Returning for self-training:\", probabilities.sum(), \"Unlabeled. Had: %d, now have: %d\"%(len(x_unlabeled), len(updated_unlabel_indiced)))\n",
    "    return np.array(new_x), np.array(new_y), x_unlabeled[updated_unlabel_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_my_multitask_nn_models():\n",
    "    inputs = Input(shape=(600,))\n",
    "    x = Dense(50, activation='sigmoid')(inputs)\n",
    "\n",
    "    coord = Dense(1, activation='sigmoid', name='coord_output')(x)\n",
    "    hyper = Dense(1, activation='sigmoid', name='hyper_output')(x)\n",
    "\n",
    "    model_coord = Model(inputs=[inputs], outputs=[coord])\n",
    "    model_hyper = Model(inputs=[inputs], outputs=[hyper])\n",
    "\n",
    "    model_coord.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    model_hyper.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    return model_coord, model_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the results may not be identical to those put to the paper due to randomization; the differences in any case should be very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for name, x_train, y_train, x_test, y_test in zip([\"Coord-Random\", \"Hyper-Random\"], [x_train_1, x_train_2], [y_train_1, y_train_2], [x_test_1, x_test_2], [y_test_1, y_test_2]):   \n",
    "    # Perform the splits in train, validation, unlabeled\n",
    "    x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train, stratify=y_train, test_size=0.6, random_state=1234,)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, stratify=y_train,  test_size=0.30, random_state=1234,)\n",
    "    print(\"Problem\", name, \"\\nBasic Stats:\", len(x_train), len(x_valid), len(x_unlabeled), len(x_test), end=\" \\n\")\n",
    "\n",
    "    # Start with the accuracy and MaF1 scores of a Dummy Classifier\n",
    "    clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_test)\n",
    "    print(\"Majority classifier:\", accuracy_score(y_test, preds), f1_score(y_test, preds, average=\"macro\"), end=\" \\n\")\n",
    "\n",
    "    \n",
    "    # Logistic Regression with default params\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_test)\n",
    "    print(\"Logistic Regression:\", accuracy_score(y_test, preds), f1_score(y_test, preds, average=\"macro\"), end=\" \\n\")\n",
    "    \n",
    "    # NN baseline\n",
    "    model = get_my_nn_model()\n",
    "    model.fit(x_train, y_train, epochs=500, validation_data=(x_valid, y_valid), verbose=False, callbacks=[EarlyStopping(patience=5)])\n",
    "    preds = model.predict_classes(x_test, verbose=False)\n",
    "    print(\"NN baseline:\", accuracy_score(y_test, preds), f1_score(y_test, preds, average=\"macro\"), end=\" \\n\")\n",
    "    \n",
    "    # keep the train/validation/test splits so that hey can be used with multitask learning and the results are comparable between them\n",
    "    data[name]={\"x_train\": x_train, \"y_train\":y_train, \"x_unlabeled\":x_unlabeled, \"y_unlabeled\":y_unlabeled,  \"x_valid\":x_unlabeled, \"y_valid\":y_unlabeled, \"x_test\":x_test,  \"y_test\":y_test } \n",
    "    \n",
    "    # NN baseline + self learning \n",
    "    accuracy_scores, mif_scores = [], []\n",
    "    for i in range(20):\n",
    "        tmpx, tmpy, x_unlabeled = get_n_most_confident_predictions(model.predict(x_unlabeled), 10, y_train, x_unlabeled) # use 10 labels each time\n",
    "        x_train = np.vstack((x_train, tmpx))\n",
    "        y_train = np.concatenate((y_train, tmpy))\n",
    "        model = get_my_nn_model()\n",
    "        model.fit(x_train, y_train, nb_epoch=500, validation_data=(x_valid, y_valid), verbose=False, \\\n",
    "                  callbacks=[EarlyStopping(patience=5)],)\n",
    "        preds = model.predict_classes(x_test, verbose=0)\n",
    "        accuracy_scores.append(accuracy_score(y_test, preds))# , f1_score(y_test, preds, average=\"macro\"))        \n",
    "        mif_scores.append( f1_score(y_test, preds, average=\"macro\") )\n",
    "    \n",
    "    print(\"NN baseline + self-learning\", max(accuracy_scores), max(mif_scores)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat the process with multi-task learning\n",
    "model_coord, model_hyper = get_my_multitask_nn_models()\n",
    "\n",
    "acc_scores_coord,mif_scores_coord, acc_scores_hyper,  mif_scores_hyper= [], [], [], []\n",
    "\n",
    "for epoch in range(250):\n",
    "    model_coord.fit(data[\"Coord-Random\"][\"x_train\"], data[\"Coord-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "    model_hyper.fit(data[\"Hyper-Random\"][\"x_train\"], data[\"Hyper-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "    preds_coord = (model_coord.predict(data[\"Coord-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "    preds_hyper = (model_hyper.predict(data[\"Hyper-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "    preds_coord_valid = (model_coord.predict(data[\"Coord-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "    preds_hyper_valid = (model_hyper.predict(data[\"Hyper-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "    acc_scores_coord.append([accuracy_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid), accuracy_score(data[\"Coord-Random\"][\"y_test\"], preds_coord)])\n",
    "    acc_scores_hyper.append([accuracy_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid), accuracy_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper)])\n",
    "    mif_scores_coord.append([f1_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid, average=\"macro\") , f1_score(data[\"Coord-Random\"][\"y_test\"], preds_coord, average=\"macro\")])\n",
    "    mif_scores_hyper.append([f1_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid, average=\"macro\") , f1_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper, average=\"macro\")])\n",
    "\n",
    "# Monitor validation score in the score tuple and print accordingly\n",
    "print(\"Multitask (Coord-Random):\", max(acc_scores_coord)[1], max(mif_scores_coord)[1], \"\\nMultitask (Hyper-Random):\",  max(acc_scores_hyper)[1], max(mif_scores_hyper)[1])\n",
    "\n",
    "\n",
    "acc_scores_coord,mif_scores_coord, acc_scores_hyper,  mif_scores_hyper= [], [], [], []\n",
    "for i in range(20):\n",
    "    tmpx, tmpy, data[\"Coord-Random\"][\"x_unlabeled\"] = get_n_most_confident_predictions(model_coord.predict(data[\"Coord-Random\"][\"x_unlabeled\"]), 10, data[\"Coord-Random\"][\"y_train\"], data[\"Coord-Random\"][\"x_unlabeled\"])\n",
    "    data[\"Coord-Random\"][\"x_train\"] = np.vstack((data[\"Coord-Random\"][\"x_train\"], tmpx))\n",
    "    data[\"Coord-Random\"][\"y_train\"] = np.concatenate((data[\"Coord-Random\"][\"y_train\"], tmpy))\n",
    "\n",
    "    tmpx, tmpy, data[\"Hyper-Random\"][\"x_unlabeled\"] = get_n_most_confident_predictions(model_hyper.predict(data[\"Hyper-Random\"][\"x_unlabeled\"]), 10, data[\"Hyper-Random\"][\"y_train\"], data[\"Hyper-Random\"][\"x_unlabeled\"])\n",
    "    data[\"Hyper-Random\"][\"x_train\"] = np.vstack((data[\"Hyper-Random\"][\"x_train\"], tmpx))\n",
    "    data[\"Hyper-Random\"][\"y_train\"] = np.concatenate((data[\"Hyper-Random\"][\"y_train\"], tmpy))\n",
    "    acc_scores_coord_, mif_scores_coord_, acc_scores_hyper_,  mif_scores_hyper_ = [], [], [], []\n",
    "\n",
    "    model_coord, model_hyper = get_my_multitask_nn_models()\n",
    "\n",
    "    for epoch in range(500):\n",
    "        model_coord.fit(data[\"Coord-Random\"][\"x_train\"], data[\"Coord-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "        model_hyper.fit(data[\"Hyper-Random\"][\"x_train\"], data[\"Hyper-Random\"][\"y_train\"], epochs=1, validation_data=None, verbose=False, )\n",
    "        preds_coord = (model_coord.predict(data[\"Coord-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "        preds_hyper = (model_hyper.predict(data[\"Hyper-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "        preds_coord_valid = (model_coord.predict(data[\"Coord-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "        preds_hyper_valid = (model_hyper.predict(data[\"Hyper-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "        acc_scores_coord_.append([accuracy_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid), accuracy_score(data[\"Coord-Random\"][\"y_test\"], preds_coord)])\n",
    "        acc_scores_hyper_.append([accuracy_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid), accuracy_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper)])\n",
    "        mif_scores_coord_.append([f1_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid, average=\"macro\") , f1_score(data[\"Coord-Random\"][\"y_test\"], preds_coord, average=\"macro\")])\n",
    "        mif_scores_hyper_.append([f1_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid, average=\"macro\") , f1_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper, average=\"macro\")])\n",
    "\n",
    "\n",
    "    acc_scores_coord.append(max(acc_scores_coord_))\n",
    "    mif_scores_coord.append(max(mif_scores_coord_))\n",
    "    acc_scores_hyper.append(max(acc_scores_hyper_))\n",
    "    mif_scores_hyper.append(max(mif_scores_hyper_))\n",
    "print(\"\\nMultitask + Self-learning(Coord-Random):\", max(acc_scores_coord)[1], max(mif_scores_coord)[1], \"\\nMultitask + Self-learning(Hyper-Random):\",  max(acc_scores_hyper)[1],  max(mif_scores_hyper)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
