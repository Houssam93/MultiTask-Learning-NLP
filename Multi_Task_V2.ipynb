{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import scipy.spatial as sp\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "import numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Add, concatenate , Subtract\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization,Lambda\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import json\n",
    "# np.random.seed(44)\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to load embeddings\n",
    "def load_embeddings(path, dimension):\n",
    "    f = open(path, encoding=\"utf8\").read().splitlines()\n",
    "    vectors = {}\n",
    "    for i in f:\n",
    "        elems = i.split()\n",
    "        vectors[\" \".join(elems[:-dimension])] =  np.array(elems[-dimension:]).astype(float)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Embeddings\n",
    "embeddings = load_embeddings(\"glove.6B.300d.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "#We include the concatenation, the difference or Cosine. All the combinations are possible.\n",
    "\n",
    "def get_vector_representation_of_word_pairs(dataframe, embeddings_voca,concat,diff,Cosine):\n",
    "    x1 = [embeddings_voca[word] for word in dataframe.w1.values]\n",
    "    x2 =[embeddings_voca[word] for word in dataframe.w2.values]\n",
    "    y = dataframe.Category.values\n",
    "    #Concatenation\n",
    "    if concat :\n",
    "        x = np.hstack((x1, x2))\n",
    "    #Difference\n",
    "    if diff :\n",
    "        z=np.subtract(x1, x2)\n",
    "        if concat :\n",
    "            x= np.hstack((x,z))\n",
    "        else :\n",
    "            x=z\n",
    "    #Cosine\n",
    "    if Cosine :\n",
    "        cosine=np.diag(1 - sp.distance.cdist(x1,x2, 'cosine'))\n",
    "        cosine=np.reshape(cosine,(-1,1))\n",
    "        x= np.hstack((x,cosine)) \n",
    "\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perpare_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "#Read the data and the Output is 3 Dataframes : df1 for task1 ,df2 for task2 and df3 for Random Pairs\n",
    "def three_data(Rumen,Root9,Bless,Cogalex,Weeds):\n",
    "    def prep_df(df):\n",
    "        cols =df.columns\n",
    "        for i in range(len(cols)) :\n",
    "            for j in range(len(df)):\n",
    "                df[df.columns[i]].values[j]=df[df.columns[i]].values[j][0:-2]\n",
    "        return df\n",
    "    if Weeds:\n",
    "        Rumen=True\n",
    "        task1=\"HYPER\"\n",
    "        task2=\"Coord\"\n",
    "        link2=\"data/coordpairs2_wiki100.json\"\n",
    "        link1=\"data/entpairs2_wiki100.json\"\n",
    "    if Cogalex :\n",
    "        task1=\"HYPER\"\n",
    "        task2=\"SYN\"\n",
    "        link1=\"CogALexV_train_v1/gold_task2.txt\"\n",
    "        link2=\"CogALexV_test_v1/gold_task2.txt\"\n",
    "    if Rumen :\n",
    "        task1=\"HYPER\"\n",
    "        task2=\"SYN\"\n",
    "        link=\"RumenPairs.txt\"\n",
    "    if Root9 :\n",
    "        link_hyper=\"MULTITASK-LEARNING/ROOT9/ROOT9_hyper.txt\"\n",
    "        link_coord=\"MULTITASK-LEARNING/ROOT9/ROOT9_coord.txt\"\n",
    "        link_random=\"MULTITASK-LEARNING/ROOT9/ROOT9_random.txt\"\n",
    "        task1= \"HYPER\"\n",
    "        task2= \"COORD\"\n",
    "    if Bless :\n",
    "        task1= \"HYPER\"\n",
    "        task2= \"MERO\"\n",
    "        link_coord=\"MULTITASK-LEARNING/BLESS/BLESS_mero.txt\"\n",
    "        link_hyper=\"MULTITASK-LEARNING/BLESS/BLESS_hyper.txt\"\n",
    "        link_random=\"MULTITASK-LEARNING/BLESS/BLESS_random.txt\"\n",
    "\n",
    "    def get_names(cat):\n",
    "        if cat == 0 : return \"RANDOM\"\n",
    "        if cat == 1: return task1\n",
    "        if cat == 2: return task2\n",
    "    def get_names_Weeds1(cat):\n",
    "        if cat == 0 : return \"RANDOM\"\n",
    "        if cat == 1: return task1\n",
    "    def get_names_Weeds2(cat):\n",
    "        if cat == 0 : return \"RANDOM\"\n",
    "        if cat == 1: return task2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if Rumen :\n",
    "        dff = pd.read_csv(link)\n",
    "        dff.rename(columns={\"W1\":\"w1\", \"W2\":\"w2\",\"rel\":\"Category\"}, inplace=True)\n",
    "        dff[\"Category\"] = dff[\"Category\"].apply(get_names)\n",
    "        df = dff.loc[dff.Category == task2]\n",
    "        df2 = dff.loc[dff.Category == task1]\n",
    "        df3 = dff.loc[dff.Category == \"RANDOM\"]\n",
    "        #print(len(df),len(df2),len(df3))\n",
    "    if Root9 or Bless:\n",
    "\n",
    "        df = pd.read_csv(link_coord,header=None,sep = '\\t')\n",
    "        df.rename(index=str,columns={0:\"w1\", 2:\"w2\",1:\"Category\"},inplace=True)\n",
    "        df=prep_df(df)\n",
    "        df2 = pd.read_csv(link_hyper,header=None,sep = '\\t')\n",
    "        df2.rename(index=str,columns={0:\"w1\", 2:\"w2\",1:\"Category\"},inplace=True)\n",
    "        df2=prep_df(df2)\n",
    "        df3 = pd.read_csv(link_random,header=None,sep = '\\t')\n",
    "        df3.rename(index=str,columns={0:\"w1\", 2:\"w2\",1:\"Category\"},inplace=True)\n",
    "        df3=prep_df(df3)\n",
    "    if Cogalex:\n",
    "\n",
    "        dff1 = pd.read_csv(link1,header=None,sep = '\\t')\n",
    "        dff2 = pd.read_csv(link2,header=None,sep = '\\t')\n",
    "        dff1.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff2.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff3=pd.concat([dff1,dff2])\n",
    "        #print(list(set(dff3.Category.values.tolist())))\n",
    "        df = dff3.loc[dff3.Category == task2]\n",
    "        df2 = dff3.loc[dff3.Category == task1]\n",
    "        df3 = dff3.loc[dff3.Category == \"RANDOM\"]\n",
    "    if Weeds :\n",
    "        json_data=open(link1).read()\n",
    "        data = json.loads(json_data)\n",
    "        dff=pd.DataFrame(data)\n",
    "        dff.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff[\"Category\"] = dff[\"Category\"].apply(get_names_Weeds1)\n",
    "        df2 = dff.loc[dff.Category == task1]\n",
    "        #df3 = dff.loc[dff.Category == \"RANDOM\"]\n",
    "        #df3=df3[0:len(df2)]\n",
    "        #print(\"taille 0,1 pour entpairs\",len(df2),len(df3))\n",
    "        \n",
    "        json_data2=open(link2).read()\n",
    "        data2 = json.loads(json_data2)\n",
    "        dff2=pd.DataFrame(data2)\n",
    "        dff2.rename(index=str,columns={0:\"w1\", 1:\"w2\",2:\"Category\"},inplace=True)\n",
    "        dff2[\"Category\"] = dff2[\"Category\"].apply(get_names_Weeds2)\n",
    "        df = dff2.loc[dff2.Category == task2]\n",
    "        #df=df[0:len(df2)]\n",
    "        #print(\"taille 0,1 pour coord\",len(df))\n",
    "        \n",
    "        \n",
    "    return df,df2,df3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct the train and test data based on Lexical Split\n",
    "def func_train_test(name_data,embeddings,concat,diff,cosine):\n",
    "    \n",
    "    Cogalex,Rumen,Root9,Weeds,Bless=False,False,False,False,False\n",
    "    if str(name_data)==\"Cogalex\" :\n",
    "        Cogalex=True\n",
    "        #print('Preprocessing of Cogalex ')\n",
    "    if str(name_data)==\"Weeds\" :\n",
    "        Weeds=True\n",
    "        #print('Preprocessing of Weeds ')\n",
    "        \n",
    "    if str(name_data)==\"Rumen\" :\n",
    "        Rumen=True\n",
    "        #print('Preprocessing of Rumen ')\n",
    "    if str(name_data)==\"Root9\" :\n",
    "        Root9=True\n",
    "        #print('Preprocessing of Root9 ')\n",
    "    if str(name_data)==\"Bless\" :\n",
    "        Bless=True\n",
    "        #print('Preprocessing of Bless')\n",
    "        \n",
    "    df,df2,df3=three_data(Rumen,Root9,Bless,Cogalex,Weeds)\n",
    " \n",
    "    words_coord = list(set(df.w1.values.tolist() + df.w2.values.tolist()))\n",
    "    words_hyper = list(set(df2.w1.values.tolist() + df2.w2.values.tolist()))\n",
    "    words_random = list(set(df3.w1.values.tolist() + df3.w2.values.tolist()))\n",
    "\n",
    "    words_ = sorted(list(set(words_coord+words_hyper+words_random)))\n",
    "    #words_ = sorted(list(set(words_coord+words_hyper+words_random1+words_random2)))\n",
    "    \n",
    "    #LExical SPlit\n",
    "    words_train, words_test =train_test_split(words_, test_size=0.4, random_state=1344)\n",
    "    #print('numbers_words_task1_not_sorted: '+str(len(list((df.w1.values.tolist() + df.w2.values.tolist())))))\n",
    "    #print('numbers_words_sorted: '+str(len(words_)) )\n",
    "    #print('numbers_words_task1_sorted: '+str(len(words_coord)))\n",
    "    #print('numbers_words_task2_sorted: '+str(len(words_hyper)) )\n",
    "    #print('numbers_words_random_sorted: '+str(len(words_random)) +'\\n')\n",
    "\n",
    "    \n",
    "    #print('numbers_pairs_task1_all: '+str(len(df)) )\n",
    "    #print('numbers_pairs_task2_all: '+str(len(df2)) )\n",
    "    #print('numbers_pairs_random_all: '+str(len(df3)) +'\\n')\n",
    "  \n",
    "\n",
    "    df[\"known_words\"] = df.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )\n",
    "    df2[\"known_words\"] = df2.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )\n",
    "    df3[\"known_words\"] = df3.apply(lambda l: l[\"w1\"] in embeddings and l[\"w2\"] in embeddings, axis =1  )\n",
    "    \n",
    "    \n",
    "    #print('numbers_pairs_task1_known: '+str(len(df)) )\n",
    "    #print('numbers_pairs_task2_known: '+str(len(df2)) )\n",
    "    #print('numbers_pairs_random_known: '+str(len(df3)) +'\\n')\n",
    "    \n",
    "    \n",
    "    # Given the words in the train and test parts, mark the pairs as training or testing, when both words of aa pair belong to the train or test vocabulary.\n",
    "    df[\"is_train\"] = df.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "    df[\"is_test\"] = df.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    df2[\"is_train\"] = df2.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "    df2[\"is_test\"] = df2.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    " \n",
    "    \n",
    "    df3[\"is_train\"] = df3.apply(lambda l : l[\"w1\"] in words_train and l[\"w2\"] in words_train and l[\"known_words\"] == True, axis=1 )\n",
    "    df3[\"is_test\"] = df3.apply(lambda l : l[\"w1\"] in words_test and l[\"w2\"] in words_test and l[\"known_words\"] == True, axis=1)\n",
    "\n",
    "    xtrainCoord, ytrainCoord = get_vector_representation_of_word_pairs(df.loc[df.is_train==True], embeddings,concat,diff,cosine)\n",
    "    xtestCoord, ytestCoord   = get_vector_representation_of_word_pairs(df.loc[df.is_test==True], embeddings,concat,diff,cosine)\n",
    "    \n",
    "    #print('numbers_pairs_task1_train: '+str(len(ytrainCoord) ))\n",
    "    #print('numbers_pairs_task1_test: '+str(len(ytestCoord)) )\n",
    "    #print('Percentage_pairs_train/test: '+str(len(ytestCoord)*100/(len(ytestCoord)+len(ytrainCoord)))+ '%'+'\\n') \n",
    "    \n",
    "    \n",
    "\n",
    "    xtrainHyper, ytrainHyper = get_vector_representation_of_word_pairs(df2.loc[df2.is_train==True], embeddings,concat,diff,cosine)\n",
    "    xtestHyper, ytestHyper   = get_vector_representation_of_word_pairs(df2.loc[df2.is_test==True], embeddings,concat,diff,cosine)\n",
    "          \n",
    "    #print('numbers_pairs_task2_train: '+str(len(ytrainHyper) ))\n",
    "    #print('numbers_pairs_task2_test: '+str(len(ytestHyper)) )\n",
    "    #print('Percentage_pairs_train/test: '+str(len(ytestHyper)*100/(len(ytestHyper)+len(ytrainHyper)))+ '%'+'\\n') \n",
    "          \n",
    "          \n",
    "          \n",
    "    xtrainRando, ytrainRando = get_vector_representation_of_word_pairs(df3.loc[df3.is_train==True], embeddings,concat,diff,cosine) \n",
    "    xtestRando, ytestRando   = get_vector_representation_of_word_pairs(df3.loc[df3.is_test==True], embeddings,concat,diff,cosine)\n",
    "    \n",
    "    #print('numbers_pairs_random_train: '+str(len(ytrainRando) ))\n",
    "    #print('numbers_pairs_random_test: '+str(len(ytestRando)) )\n",
    "    #print('Percentage_random_train/test: '+str(len(ytestRando)*100/(len(ytestRando)+len(ytrainRando)))+ '%'+'\\n') \n",
    "          \n",
    "    x_train_1, x_train_2 = np.vstack((xtrainCoord, xtrainRando)), np.vstack((xtrainHyper, xtrainRando))\n",
    "    y_train_1, y_train_2 = [1]*len(xtrainCoord) + [0]*len(xtrainRando), [1]*len(xtrainHyper) + [0]*len(xtrainRando)\n",
    "\n",
    "\n",
    "\n",
    "    x_test_1, x_test_2 = np.vstack((xtestCoord, xtestRando)), np.vstack((xtestHyper, xtestRando))\n",
    "    y_test_1, y_test_2 = [1]*len(xtestCoord) + [0]*len(xtestRando), [1]*len(xtestHyper) + [0]*len(xtestRando)\n",
    "    \n",
    "\n",
    "\n",
    "    x_train_1, y_train_1 = shuffle(x_train_1, y_train_1, random_state=1234)\n",
    "    x_train_2, y_train_2 = shuffle(x_train_2, y_train_2, random_state=1234)\n",
    "    x_test_1, y_test_1 = shuffle(x_test_1, y_test_1, random_state=1234)\n",
    "    x_test_2, y_test_2 = shuffle(x_test_2, y_test_2, random_state=1234)\n",
    "    assert len(x_train_1) == len(y_train_1)\n",
    "    assert len(x_train_2) == len(y_train_2)\n",
    "    assert len(x_test_1) == len(y_test_1)\n",
    "    assert len(x_test_2) == len(y_test_2)\n",
    "    data = {}\n",
    "    for name, x_train, y_train, x_test, y_test in zip([\"Coord-Random\",\"Hyper-Random\"], [x_train_1, x_train_2], [y_train_1, y_train_2], [x_test_1, x_test_2], [y_test_1, y_test_2]):   \n",
    "        # Perform the splits in train, validation, unlabeled\n",
    "        #x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train, stratify=y_train, test_size=0.6, random_state=1234,)\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, stratify=y_train,  test_size=0.30, random_state=1234,)\n",
    "\n",
    "\n",
    "        # keep the train/validation/test splits so that hey can be used with multitask learning and the results are comparable between them\n",
    "        #print('name '+name)\n",
    "        data[name]={\"x_train\": x_train, \"y_train\":y_train,   \"x_valid\":x_valid, \"y_valid\":y_valid, \"x_test\":x_test,  \"y_test\":y_test } \n",
    "    \n",
    "    sum_task1Vsrandom=len(data[\"Coord-Random\"][\"y_train\"])+len(data[\"Coord-Random\"][\"y_test\"])+len(data[\"Coord-Random\"][\"y_valid\"])\n",
    "    sum_task2Vsrandom=len(data[\"Hyper-Random\"][\"y_train\"])+len(data[\"Hyper-Random\"][\"y_test\"])+len(data[\"Hyper-Random\"][\"y_valid\"])\n",
    "    #print('sums_pairs_task1Vsrandom :' +str(sum_task1Vsrandom) )\n",
    "    #print('sums_pairs_task2Vsrandom :' +str(sum_task2Vsrandom) +'\\n')\n",
    "        \n",
    "    #print('numbers_pairs_task1Vsrandom_train: '+str(len(data[\"Coord-Random\"][\"y_train\"]) ), 'Percentage :'+ str(len(data[\"Coord-Random\"][\"y_train\"])*100/sum_task1Vsrandom) +' %')\n",
    "    #print('numbers_pairs_task2Vsrandom_train: '+str(len(data[\"Hyper-Random\"][\"y_train\"]) ), 'Percentage :'+ str(len(data[\"Hyper-Random\"][\"y_train\"])*100/sum_task2Vsrandom) +' %'+'\\n')\n",
    "    \n",
    "    #print('numbers_pairs_task1Vsrandom_valid: '+str(len(data[\"Coord-Random\"][\"y_valid\"]) ),'Percentage :'+ str(len(data[\"Coord-Random\"][\"y_valid\"])*100/sum_task1Vsrandom) +' %')\n",
    "    #print('numbers_pairs_task2Vsrandom_valid: '+str(len(data[\"Hyper-Random\"][\"y_valid\"]) ) , 'Percentage :'+ str(len(data[\"Hyper-Random\"][\"y_valid\"])*100/sum_task2Vsrandom) +' %'+'\\n')\n",
    "    \n",
    "    #print('numbers_pairs_task1Vsrandom_test: '+str(len(data[\"Coord-Random\"][\"y_test\"]) ),'Percentage :'+ str(len(data[\"Coord-Random\"][\"y_test\"])*100/sum_task1Vsrandom) +' %')\n",
    "    #print('numbers_pairs_task2Vsrandom_test: '+str(len(data[\"Hyper-Random\"][\"y_test\"]) ), 'Percentage :'+ str(len(data[\"Hyper-Random\"][\"y_test\"])*100/sum_task2Vsrandom) +' %'+'\\n')\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These function gives the input dim. It should be imporoved in the future.\n",
    "def input_dim_func(concat,diff,cosine):\n",
    "    if concat and diff== False  and cosine==False   :\n",
    "        input_dim=600\n",
    "    if concat and diff== False  and cosine   :\n",
    "        input_dim=601\n",
    "    if concat and diff and cosine==False   :\n",
    "        input_dim=900\n",
    "    if concat and diff and cosine   :\n",
    "        input_dim=901\n",
    "    if concat==False and diff and cosine==False   :\n",
    "        input_dim=300\n",
    "    if concat==False and diff  and cosine  :\n",
    "        input_dim=301\n",
    "    return input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simple NN baseline\n",
    "def get_my_nn_model():\n",
    "    input_dim=input_dim_func(concat,diff,cosine)\n",
    "    #print('NN_baseline','input_dim ',input_dim)\n",
    "    \"\"\"Defines the NN baseline.\n",
    "    Two hidden layers, followed by the output layer. \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, activation='sigmoid', input_dim=input_dim))\n",
    "    #model.add(Dropout(0.1))\n",
    "    #model.add(Dense(15, activation='sigmoid'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "#Multi_task LEarning V0.0\n",
    "def get_my_multitask_nn_models_ancien():\n",
    "    #print('Multi Ancien')\n",
    "    input_dim=input_dim_func(concat,diff,cosine)\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(50, activation='sigmoid')(inputs)\n",
    "    \n",
    "   \n",
    "    coord = Dense(1, activation='sigmoid', name='coord_output')(x)\n",
    "    #coord=Dropout(0.05)(coord)\n",
    "    hyper = Dense(1, activation='sigmoid', name='hyper_output')(x)\n",
    "    #hyper=Dropout(0.5)(hyper)\n",
    "    \n",
    "    model_coord = Model(inputs=[inputs], outputs=[coord])\n",
    "    model_hyper = Model(inputs=[inputs], outputs=[hyper])\n",
    "\n",
    "    model_coord.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    model_hyper.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    return model_coord, model_hyper\n",
    "\n",
    "#Multi task Learning V1.1\n",
    "def get_my_multitask_nn_models():\n",
    " \n",
    "    input_dim=input_dim_func(concat,diff,cosine)\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    #The sared layer\n",
    "    x = Dense(300, activation='relu')(inputs)\n",
    "\n",
    "                \n",
    "\n",
    "    #The layer of task1\n",
    "    concatenated_hyper = concatenate([inputs,x])\n",
    "    #The layer of task2\n",
    "    concatenated_coord = concatenate([inputs,x])\n",
    "\n",
    "    \n",
    "    xcoord = Dense(50, activation='sigmoid')(concatenated_coord)\n",
    "    xhyper= Dense(50, activation='sigmoid')(concatenated_hyper)\n",
    "\n",
    "    \n",
    "    #Output for task1\n",
    "    hyper = Dense(1, activation='sigmoid', name='hyper_output')(xhyper)\n",
    "    model_hyper = Model(inputs=[inputs], outputs=[hyper])    \n",
    "                \n",
    "    #Output for task2\n",
    "    coord = Dense(1, activation='sigmoid', name='coord_output')(xcoord)\n",
    "    model_coord = Model(inputs=[inputs], outputs=[coord])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    model_coord.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    model_hyper.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    return model_coord, model_hyper\n",
    "\n",
    "\n",
    "#Multi Task V1.1\n",
    "def get_my_multitask_nn_models_new():\n",
    "    #print('Multi New2')\n",
    "    input_dim=input_dim_func(concat,diff,cosine)\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    #This version can be useful only if we have the concatenation for the 2 words.\n",
    "    if input_dim<600 :\n",
    "        model_coord, model_hyper= get_my_multitask_nn_models()\n",
    "        return model_coord, model_hyper\n",
    "    else :    \n",
    "    #xcoord0 = Dense(50, activation='sigmoid',kernel_initializer='zeros',bias_initializer='zeros')(inputs)\n",
    "    #xcoord = Dense(300, activation='sigmoid')(inputs)\n",
    "    #X1=Dense(25, activation='sigmoid')(xcoord)\n",
    "    #inputs1=inputs[0:300]\n",
    "    #inputs2=inputs[300:600]\n",
    "        inputs1=Lambda(lambda x: x[:,0:300])(inputs)\n",
    "\n",
    "        inputs2=Lambda(lambda x: x[:,300:600])(inputs)\n",
    "        inputs3=Lambda(lambda x: x[:,600:])(inputs)\n",
    "        x = Dense(300, activation='relu')(inputs)\n",
    "        x1= Dense(100, activation='relu')(inputs1)\n",
    "        x2= Dense(100, activation='relu')(inputs2)\n",
    "\n",
    "\n",
    "        concatenated_hyper = concatenate([inputs,x])       \n",
    "        concatenated_hyper1 = concatenate([concatenated_hyper,x1])\n",
    "        concatenated_hyper2 = concatenate([concatenated_hyper1,x2])\n",
    "        concatenated_hyper3 = concatenate([concatenated_hyper2,inputs3])\n",
    "\n",
    "\n",
    "        concatenated_coord = concatenate([inputs,x])\n",
    "        concatenated_coord1 = concatenate([concatenated_coord,x1])\n",
    "        concatenated_coord2 = concatenate([concatenated_coord1,x2])\n",
    "        concatenated_coord3 = concatenate([concatenated_coord2,inputs3])\n",
    "        #concatenated = Add()([xcoord,xhyper])\n",
    "        #concatenated = Subtract()([xcoord,xhyper])\n",
    "        xcoord = Dense(50, activation='sigmoid')(concatenated_coord3)\n",
    "        xhyper= Dense(50, activation='sigmoid')(concatenated_hyper3)\n",
    "        #xcommun= Dense(50,activation='sigmoid')(concatenated)\n",
    "        #xafter= Dense(25,activation='sigmoid')(xcommun)\n",
    "\n",
    "\n",
    "        #hyper0 = Dense(5, activation='sigmoid')(concatenated_hyper)\n",
    "        hyper = Dense(1, activation='sigmoid', name='hyper_output')(xhyper)\n",
    "        #hyper=Dropout(0.5)(hyper)\n",
    "\n",
    "\n",
    "        model_hyper = Model(inputs=[inputs], outputs=[hyper])    \n",
    "\n",
    "\n",
    "        #coord0 = Dense(5, activation='sigmoid')(concatenated_coord)\n",
    "        coord = Dense(1, activation='sigmoid', name='coord_output')(xcoord)\n",
    "        model_coord = Model(inputs=[inputs], outputs=[coord])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model_coord.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "        model_hyper.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "        return model_coord, model_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(1)\n",
    "##############1\n",
    "#Here num_epochs is the number of itreation. Every iteration see one batch.\n",
    "num_epochs=3000\n",
    "#seuil = 50\n",
    "batch=64\n",
    "\n",
    "#hyp=True\n",
    "#Coord=True\n",
    "import time\n",
    "\n",
    "#Train Model\n",
    "def train_model(data,name_model,Multi=False):\n",
    "    tmps1=time.time()\n",
    "\n",
    "    if Multi==False :\n",
    "        model_coord = name_model()\n",
    "        model_hyper = name_model()\n",
    "    else : model_coord, model_hyper = name_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    acc_scores_coord,mif_scores_coord, acc_scores_hyper,  mif_scores_hyper= [], [], [], []\n",
    "    precision_score_coord,precision_score_hyper,recall_score_hyper,recall_score_coord=[],[],[],[]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch%500==0 :\n",
    "            #print (epoch)\n",
    "            tmps3=time.time()-tmps1\n",
    "            #print (\"Temps d'execution = %f\" %tmps3 + \"s\")\n",
    "        a1=len(data[\"Hyper-Random\"][\"x_train\"])\n",
    "        a2=len(data[\"Coord-Random\"][\"x_train\"])\n",
    "\n",
    "        idx1 = np.random.choice(np.arange(a1), batch, replace=False)\n",
    "        idx2 = np.random.choice(np.arange(a2), batch, replace=False)\n",
    "       \n",
    "        model_hyper.fit(data[\"Hyper-Random\"][\"x_train\"][idx1], np.array(data[\"Hyper-Random\"][\"y_train\"])[idx1], epochs=1, validation_data=None, verbose=False, )\n",
    "        model_coord.fit(data[\"Coord-Random\"][\"x_train\"][idx2], np.array(data[\"Coord-Random\"][\"y_train\"])[idx2], epochs=1, validation_data=None, verbose=False, )\n",
    "        #We save the results after seeing 100 batches.\n",
    "        if epoch%100==0 :\n",
    "            preds_coord = (model_coord.predict(data[\"Coord-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "            preds_hyper = (model_hyper.predict(data[\"Hyper-Random\"][\"x_test\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "            preds_coord_valid = (model_coord.predict(data[\"Coord-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "            preds_hyper_valid = (model_hyper.predict(data[\"Hyper-Random\"][\"x_valid\"], verbose=0)> 0.5).astype(int)\n",
    "\n",
    "            acc_scores_coord.append([accuracy_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid), accuracy_score(data[\"Coord-Random\"][\"y_test\"], preds_coord)])\n",
    "            precision_score_coord.append([precision_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid), precision_score(data[\"Coord-Random\"][\"y_test\"], preds_coord)])\n",
    "            recall_score_coord.append([recall_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid), recall_score(data[\"Coord-Random\"][\"y_test\"], preds_coord)])\n",
    "            acc_scores_hyper.append([accuracy_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid), accuracy_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper)])\n",
    "            precision_score_hyper.append([precision_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid), precision_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper)])\n",
    "            recall_score_hyper.append([recall_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid), recall_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper)])\n",
    "\n",
    "            mif_scores_coord.append([f1_score(data[\"Coord-Random\"][\"y_valid\"], preds_coord_valid, average=\"macro\") , f1_score(data[\"Coord-Random\"][\"y_test\"], preds_coord, average=\"macro\")])\n",
    "            mif_scores_hyper.append([f1_score(data[\"Hyper-Random\"][\"y_valid\"], preds_hyper_valid, average=\"macro\") , f1_score(data[\"Hyper-Random\"][\"y_test\"], preds_hyper, average=\"macro\")])\n",
    "\n",
    "    df1=pd.DataFrame(acc_scores_coord, columns = ['acc_coord_valid', 'acc_coord_test'])\n",
    "    df2=pd.DataFrame(mif_scores_coord, columns = ['mif_coord_valid', 'mif_coord_test'])\n",
    "    df3=pd.DataFrame(acc_scores_hyper, columns = ['acc_hyper_valid', 'acc_hyper_test'])\n",
    "    df4=pd.DataFrame(mif_scores_hyper, columns = ['mif_hyper_valid', 'mif_hyper_test'])\n",
    "    df5=pd.DataFrame(precision_score_coord, columns = ['prec_coord_valid', 'prec_coord_test'])\n",
    "    df6=pd.DataFrame(precision_score_hyper, columns = ['prec_hyper_valid', 'prec_hyper_test'])\n",
    "    df7=pd.DataFrame(recall_score_coord, columns = ['recall_coord_valid', 'recall_coord_test'])\n",
    "    df8=pd.DataFrame(recall_score_hyper, columns = ['recall_hyper_valid', 'recall_hyper_test'])\n",
    "    frames=[df1,df2,df3,df4,df5,df6,df7,df8]\n",
    "    df_final=pd.concat(frames,axis=1)\n",
    "    acc_coord=df_final['acc_coord_test'][np.argmax(df_final['acc_coord_valid'])]\n",
    "    acc_hyper=df_final['acc_hyper_test'][np.argmax(df_final['acc_hyper_valid'])]\n",
    "    mif_coord=df_final['mif_coord_test'][np.argmax(df_final['mif_coord_valid'])]\n",
    "    mif_hyper=df_final['mif_hyper_test'][np.argmax(df_final['mif_hyper_valid'])]\n",
    "    prec_coord=df_final['prec_coord_test'][np.argmax(df_final['prec_coord_valid'])]\n",
    "    prec_hyper=df_final['prec_hyper_test'][np.argmax(df_final['prec_hyper_valid'])]\n",
    "    #recall_coord=df_final['recall_coord_test'][np.argmax(df_final['prec_coord_valid'])]\n",
    "    #recall_hyper=df_final['recall_hyper_test'][np.argmax(df_final['prec_hyper_valid'])]\n",
    "    recall_coord=df_final['recall_coord_test'][np.argmax(df_final['recall_coord_valid'])]\n",
    "    recall_hyper=df_final['recall_hyper_test'][np.argmax(df_final['recall_hyper_valid'])]\n",
    "    tmps2=time.time()-tmps1\n",
    "    tmps2=tmps2/60\n",
    "    #new_data=[[acc_coord,'Nan'],[acc_hyper,'Nan'],[mif_coord,'Nan'],[mif_coord,'Nan'],[prec_coord,'Nan'],[prec_hyper,'Nan'],[recall_coord,'Nan'],[recall_hyper,'Nan'],[tmps2,'Nan']]\n",
    "    new_data=[acc_coord,acc_hyper,mif_coord,mif_hyper,prec_coord,prec_hyper,recall_coord,recall_hyper,tmps2]\n",
    "    df_restit=pd.DataFrame(new_data,index=['acc_coord','acc_hyp','mif_coord','mif_hyper','prec_coord','prec_hyper','recall_coord','recall_hyper','time'])\n",
    "    tmps2=time.time()-tmps1\n",
    "    tmps2=tmps2/60\n",
    "    #print (\"Temps d'execution = %f\" %tmps2 + \"min\")\n",
    "\n",
    "    return df_final ,df_restit.transpose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(name_data,name_model,Multi,index):\n",
    "    evol=[0,1,2,3,4,5]\n",
    "    index_bool=['FTF','FTT','TFF','TFT','TTF','TTT']\n",
    "\n",
    "\n",
    "    index_glob=[]\n",
    "    for i in range(len(index)):\n",
    "        for j in range(len(index_bool)):\n",
    "            index_glob.append(index[i]+' '+index_bool[j])\n",
    "    cols=['acc_coord','acc_hyp','mif_coord','mif_hyper','prec_coord','prec_hyper','recall_coord','recall_hyper','time']\n",
    "    result=[]\n",
    "    global concat\n",
    "    global diff\n",
    "    global cosine\n",
    "    for i in range(len(name_model)):\n",
    "        for j in range(len(evol)):\n",
    "            if j==0:\n",
    "                concat,diff,cosine=False,True,False\n",
    "            if j==1:\n",
    "                concat,diff,cosine=False,True,True\n",
    "            if j==2:\n",
    "                concat,diff,cosine=True,False,False\n",
    "            if j==3:\n",
    "                concat,diff,cosine=True,False,True\n",
    "            if j==4:\n",
    "                concat,diff,cosine=True,True,False\n",
    "            if j==5:\n",
    "                concat,diff,cosine=True,True,True\n",
    "            data=func_train_test(name_data,embeddings,concat,diff,cosine)\n",
    "    \n",
    "            \n",
    "            df_final,df_restit=train_model(data,name_model[i],Multi[i])\n",
    "            result.append(list(df_restit.values)[0])\n",
    "    \n",
    "    df=pd.DataFrame(result,columns=cols,index=index_glob)\n",
    "    return df\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=['NN baseline','Multi_task','Multi_task New','Multi Task New2']\n",
    "models=[get_my_nn_model,get_my_multitask_nn_models_ancien,get_my_multitask_nn_models,get_my_multitask_nn_models_new]\n",
    "list_Multi=[False,True,True,True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_coord</th>\n",
       "      <th>acc_hyp</th>\n",
       "      <th>mif_coord</th>\n",
       "      <th>mif_hyper</th>\n",
       "      <th>prec_coord</th>\n",
       "      <th>prec_hyper</th>\n",
       "      <th>recall_coord</th>\n",
       "      <th>recall_hyper</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN baseline FTF</th>\n",
       "      <td>0.675493</td>\n",
       "      <td>0.702227</td>\n",
       "      <td>0.673152</td>\n",
       "      <td>0.699170</td>\n",
       "      <td>0.646643</td>\n",
       "      <td>0.646134</td>\n",
       "      <td>0.851620</td>\n",
       "      <td>0.967436</td>\n",
       "      <td>0.621199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN baseline FTT</th>\n",
       "      <td>0.809969</td>\n",
       "      <td>0.770223</td>\n",
       "      <td>0.809929</td>\n",
       "      <td>0.766756</td>\n",
       "      <td>0.817773</td>\n",
       "      <td>0.726675</td>\n",
       "      <td>0.824451</td>\n",
       "      <td>0.758480</td>\n",
       "      <td>0.656170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN baseline TFF</th>\n",
       "      <td>0.758048</td>\n",
       "      <td>0.770809</td>\n",
       "      <td>0.758035</td>\n",
       "      <td>0.767961</td>\n",
       "      <td>0.749746</td>\n",
       "      <td>0.724582</td>\n",
       "      <td>0.780564</td>\n",
       "      <td>0.773406</td>\n",
       "      <td>0.678226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN baseline TFT</th>\n",
       "      <td>0.823988</td>\n",
       "      <td>0.794842</td>\n",
       "      <td>0.823986</td>\n",
       "      <td>0.792210</td>\n",
       "      <td>0.812695</td>\n",
       "      <td>0.749035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.797829</td>\n",
       "      <td>0.769794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN baseline TTF</th>\n",
       "      <td>0.760125</td>\n",
       "      <td>0.781360</td>\n",
       "      <td>0.759571</td>\n",
       "      <td>0.778405</td>\n",
       "      <td>0.748992</td>\n",
       "      <td>0.735526</td>\n",
       "      <td>0.920585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN baseline TTT</th>\n",
       "      <td>0.819834</td>\n",
       "      <td>0.791911</td>\n",
       "      <td>0.819834</td>\n",
       "      <td>0.789040</td>\n",
       "      <td>0.814346</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.778833</td>\n",
       "      <td>0.810040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task FTF</th>\n",
       "      <td>0.690550</td>\n",
       "      <td>0.712778</td>\n",
       "      <td>0.689462</td>\n",
       "      <td>0.710033</td>\n",
       "      <td>0.674835</td>\n",
       "      <td>0.668848</td>\n",
       "      <td>0.777429</td>\n",
       "      <td>0.712347</td>\n",
       "      <td>0.688562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task FTT</th>\n",
       "      <td>0.802181</td>\n",
       "      <td>0.759672</td>\n",
       "      <td>0.802053</td>\n",
       "      <td>0.755532</td>\n",
       "      <td>0.813043</td>\n",
       "      <td>0.718876</td>\n",
       "      <td>0.808777</td>\n",
       "      <td>0.994573</td>\n",
       "      <td>0.697170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task TFF</th>\n",
       "      <td>0.751298</td>\n",
       "      <td>0.790152</td>\n",
       "      <td>0.751268</td>\n",
       "      <td>0.787287</td>\n",
       "      <td>0.741414</td>\n",
       "      <td>0.745785</td>\n",
       "      <td>0.784744</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.754480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task TFT</th>\n",
       "      <td>0.823468</td>\n",
       "      <td>0.794842</td>\n",
       "      <td>0.823468</td>\n",
       "      <td>0.792040</td>\n",
       "      <td>0.810726</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.840125</td>\n",
       "      <td>0.997286</td>\n",
       "      <td>0.766816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task TTF</th>\n",
       "      <td>0.738837</td>\n",
       "      <td>0.786049</td>\n",
       "      <td>0.738707</td>\n",
       "      <td>0.782976</td>\n",
       "      <td>0.724308</td>\n",
       "      <td>0.742228</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.793758</td>\n",
       "      <td>0.839377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task TTT</th>\n",
       "      <td>0.814123</td>\n",
       "      <td>0.793669</td>\n",
       "      <td>0.814121</td>\n",
       "      <td>0.790736</td>\n",
       "      <td>0.807179</td>\n",
       "      <td>0.750325</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.997286</td>\n",
       "      <td>0.854943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task New FTF</th>\n",
       "      <td>0.729491</td>\n",
       "      <td>0.730950</td>\n",
       "      <td>0.729465</td>\n",
       "      <td>0.727748</td>\n",
       "      <td>0.729474</td>\n",
       "      <td>0.683916</td>\n",
       "      <td>0.748171</td>\n",
       "      <td>0.723202</td>\n",
       "      <td>1.084683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task New FTT</th>\n",
       "      <td>0.821391</td>\n",
       "      <td>0.775498</td>\n",
       "      <td>0.821382</td>\n",
       "      <td>0.772401</td>\n",
       "      <td>0.820942</td>\n",
       "      <td>0.760174</td>\n",
       "      <td>0.832811</td>\n",
       "      <td>0.754410</td>\n",
       "      <td>1.095967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task New TFF</th>\n",
       "      <td>0.792316</td>\n",
       "      <td>0.806565</td>\n",
       "      <td>0.792226</td>\n",
       "      <td>0.806649</td>\n",
       "      <td>0.791967</td>\n",
       "      <td>0.770341</td>\n",
       "      <td>0.828631</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>1.345932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task New TFT</th>\n",
       "      <td>0.832814</td>\n",
       "      <td>0.831770</td>\n",
       "      <td>0.832813</td>\n",
       "      <td>0.829354</td>\n",
       "      <td>0.830031</td>\n",
       "      <td>0.793734</td>\n",
       "      <td>0.868339</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>1.362057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task New TTF</th>\n",
       "      <td>0.798027</td>\n",
       "      <td>0.804220</td>\n",
       "      <td>0.798023</td>\n",
       "      <td>0.801031</td>\n",
       "      <td>0.804574</td>\n",
       "      <td>0.767596</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.819539</td>\n",
       "      <td>1.602401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_task New TTT</th>\n",
       "      <td>0.823468</td>\n",
       "      <td>0.824150</td>\n",
       "      <td>0.823456</td>\n",
       "      <td>0.820291</td>\n",
       "      <td>0.815544</td>\n",
       "      <td>0.806034</td>\n",
       "      <td>0.842215</td>\n",
       "      <td>0.833107</td>\n",
       "      <td>1.614223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Task New2 FTF</th>\n",
       "      <td>0.726895</td>\n",
       "      <td>0.740914</td>\n",
       "      <td>0.726895</td>\n",
       "      <td>0.734558</td>\n",
       "      <td>0.722854</td>\n",
       "      <td>0.712034</td>\n",
       "      <td>0.760711</td>\n",
       "      <td>0.989145</td>\n",
       "      <td>1.151075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Task New2 FTT</th>\n",
       "      <td>0.819315</td>\n",
       "      <td>0.781360</td>\n",
       "      <td>0.819265</td>\n",
       "      <td>0.777831</td>\n",
       "      <td>0.820704</td>\n",
       "      <td>0.769457</td>\n",
       "      <td>0.807732</td>\n",
       "      <td>0.926730</td>\n",
       "      <td>1.152525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Task New2 TFF</th>\n",
       "      <td>0.785566</td>\n",
       "      <td>0.819461</td>\n",
       "      <td>0.785556</td>\n",
       "      <td>0.815896</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>0.784574</td>\n",
       "      <td>0.924765</td>\n",
       "      <td>0.989145</td>\n",
       "      <td>1.589347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Task New2 TFT</th>\n",
       "      <td>0.833853</td>\n",
       "      <td>0.822978</td>\n",
       "      <td>0.834362</td>\n",
       "      <td>0.820461</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.792297</td>\n",
       "      <td>0.840125</td>\n",
       "      <td>0.829037</td>\n",
       "      <td>1.604429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Task New2 TTF</th>\n",
       "      <td>0.800623</td>\n",
       "      <td>0.796600</td>\n",
       "      <td>0.800618</td>\n",
       "      <td>0.794018</td>\n",
       "      <td>0.775234</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>0.811912</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>1.949212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Task New2 TTT</th>\n",
       "      <td>0.827622</td>\n",
       "      <td>0.818288</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.820291</td>\n",
       "      <td>0.826541</td>\n",
       "      <td>0.803894</td>\n",
       "      <td>0.865204</td>\n",
       "      <td>0.991859</td>\n",
       "      <td>1.941172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     acc_coord   acc_hyp  mif_coord  mif_hyper  prec_coord  \\\n",
       "NN baseline FTF       0.675493  0.702227   0.673152   0.699170    0.646643   \n",
       "NN baseline FTT       0.809969  0.770223   0.809929   0.766756    0.817773   \n",
       "NN baseline TFF       0.758048  0.770809   0.758035   0.767961    0.749746   \n",
       "NN baseline TFT       0.823988  0.794842   0.823986   0.792210    0.812695   \n",
       "NN baseline TTF       0.760125  0.781360   0.759571   0.778405    0.748992   \n",
       "NN baseline TTT       0.819834  0.791911   0.819834   0.789040    0.814346   \n",
       "Multi_task FTF        0.690550  0.712778   0.689462   0.710033    0.674835   \n",
       "Multi_task FTT        0.802181  0.759672   0.802053   0.755532    0.813043   \n",
       "Multi_task TFF        0.751298  0.790152   0.751268   0.787287    0.741414   \n",
       "Multi_task TFT        0.823468  0.794842   0.823468   0.792040    0.810726   \n",
       "Multi_task TTF        0.738837  0.786049   0.738707   0.782976    0.724308   \n",
       "Multi_task TTT        0.814123  0.793669   0.814121   0.790736    0.807179   \n",
       "Multi_task New FTF    0.729491  0.730950   0.729465   0.727748    0.729474   \n",
       "Multi_task New FTT    0.821391  0.775498   0.821382   0.772401    0.820942   \n",
       "Multi_task New TFF    0.792316  0.806565   0.792226   0.806649    0.791967   \n",
       "Multi_task New TFT    0.832814  0.831770   0.832813   0.829354    0.830031   \n",
       "Multi_task New TTF    0.798027  0.804220   0.798023   0.801031    0.804574   \n",
       "Multi_task New TTT    0.823468  0.824150   0.823456   0.820291    0.815544   \n",
       "Multi Task New2 FTF   0.726895  0.740914   0.726895   0.734558    0.722854   \n",
       "Multi Task New2 FTT   0.819315  0.781360   0.819265   0.777831    0.820704   \n",
       "Multi Task New2 TFF   0.785566  0.819461   0.785556   0.815896    0.795309   \n",
       "Multi Task New2 TFT   0.833853  0.822978   0.834362   0.820461    0.837037   \n",
       "Multi Task New2 TTF   0.800623  0.796600   0.800618   0.794018    0.775234   \n",
       "Multi Task New2 TTT   0.827622  0.818288   0.825484   0.820291    0.826541   \n",
       "\n",
       "                     prec_hyper  recall_coord  recall_hyper      time  \n",
       "NN baseline FTF        0.646134      0.851620      0.967436  0.621199  \n",
       "NN baseline FTT        0.726675      0.824451      0.758480  0.656170  \n",
       "NN baseline TFF        0.724582      0.780564      0.773406  0.678226  \n",
       "NN baseline TFT        0.749035      1.000000      0.797829  0.769794  \n",
       "NN baseline TTF        0.735526      0.920585      1.000000  0.800671  \n",
       "NN baseline TTT        0.666667      0.827586      0.778833  0.810040  \n",
       "Multi_task FTF         0.668848      0.777429      0.712347  0.688562  \n",
       "Multi_task FTT         0.718876      0.808777      0.994573  0.697170  \n",
       "Multi_task TFF         0.745785      0.784744      0.791045  0.754480  \n",
       "Multi_task TFT         0.753623      0.840125      0.997286  0.766816  \n",
       "Multi_task TTF         0.742228      0.787879      0.793758  0.839377  \n",
       "Multi_task TTT         0.750325      0.827586      0.997286  0.854943  \n",
       "Multi_task New FTF     0.683916      0.748171      0.723202  1.084683  \n",
       "Multi_task New FTT     0.760174      0.832811      0.754410  1.095967  \n",
       "Multi_task New TFF     0.770341      0.828631      0.835821  1.345932  \n",
       "Multi_task New TFT     0.793734      0.868339      0.850746  1.362057  \n",
       "Multi_task New TTF     0.767596      0.818182      0.819539  1.602401  \n",
       "Multi_task New TTT     0.806034      0.842215      0.833107  1.614223  \n",
       "Multi Task New2 FTF    0.712034      0.760711      0.989145  1.151075  \n",
       "Multi Task New2 FTT    0.769457      0.807732      0.926730  1.152525  \n",
       "Multi Task New2 TFF    0.784574      0.924765      0.989145  1.589347  \n",
       "Multi Task New2 TFT    0.792297      0.840125      0.829037  1.604429  \n",
       "Multi Task New2 TTF    0.767717      0.811912      0.820896  1.949212  \n",
       "Multi Task New2 TTT    0.803894      0.865204      0.991859  1.941172  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=main(\"Rumen\",models,list_Multi,index)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3.6",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
